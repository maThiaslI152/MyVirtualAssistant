# Owlynn AI Assistant

## Overview
Owlynn is a full-stack AI assistant project featuring a Next.js frontend and FastAPI backend with RAG (Retrieval-Augmented Generation) capabilities. The system integrates multiple AI technologies including ChromaDB for vector storage, sentence transformers for embeddings, and LLM integration for intelligent conversations.

## Architecture

### Backend (FastAPI)
- **API Framework**: FastAPI with async/await support
- **Vector Database**: ChromaDB for document embeddings and similarity search
- **Embeddings**: HuggingFace Transformers with `intfloat/multilingual-e5-large` model
- **LLM Integration**: Configured for streaming responses with `Qwen3-14b` model
- **Caching**: Redis for performance optimization
- **Database**: PostgreSQL for persistent storage
- **GPU Support**: MPS (Metal Performance Shaders) acceleration on Apple Silicon

### Frontend (Next.js)
- **Framework**: Next.js with TypeScript
- **UI Components**: Modern React components
- **Real-time Communication**: Chat interface with streaming support

### Infrastructure
- **Containerization**: Docker Compose for service orchestration
- **Services**: Frontend, Backend, PostgreSQL, Redis, ChromaDB
- **Development**: Local development with hot-reload support

## Current Status

### âœ… Completed Features
- [x] Docker Compose setup for all services
- [x] PostgreSQL database with initialization scripts
- [x] Redis caching service
- [x] ChromaDB vector database integration
- [x] RAG search service with document processing
- [x] Content processor with multiple NLP capabilities
- [x] File processor supporting multiple formats (PDF, DOCX, TXT, etc.)
- [x] MPS GPU acceleration for Apple Silicon
- [x] Sentence transformers for multilingual embeddings
- [x] Chat API with streaming support
- [x] Frontend build system and Docker integration

### ðŸ”§ In Progress
- [ ] ChromaDB connection stabilization
- [ ] Backend service health checks
- [ ] LLM endpoint integration testing
- [ ] Frontend-backend API integration

## Quick Start

### Prerequisites
- Docker and Docker Compose
- Python 3.11+ (for local backend development)
- Node.js 18+ (for local frontend development)

### Using Docker (Recommended)
```bash
# Clone the repository
git clone <repository-url>
cd Owlynn

# Start all services
docker-compose up --build

# Services will be available at:
# Frontend: http://localhost:3000
# ChromaDB: http://localhost:8000
# PostgreSQL: localhost:5432
# Redis: localhost:6379
```

### Local Development

#### Backend Setup
```bash
cd backend

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Run the backend (on port 8001 to avoid ChromaDB conflict)
uvicorn main:app --reload --port 8001
```

#### Frontend Setup
```bash
cd frontend

# Install dependencies
npm install

# Build the application
npm run build

# Start the frontend
npm start
```

## Configuration

### Environment Variables
Create a `.env` file in the backend directory:
```env
OPENAI_API_KEY=your-openai-api-key-here
REDIS_URL=redis://localhost:6379
DATABASE_URL=postgresql://owlynn:owlynnpass@localhost:5432/owlynn
```

### ChromaDB Settings
The system is configured to use ChromaDB with:
- REST API implementation
- Persistent storage in Docker volumes
- Multilingual embedding support
- Document collection management

## API Endpoints

### Core Endpoints
- `GET /api/ping` - Health check
- `POST /api/chat` - Chat with streaming support
- `POST /api/rag/search` - RAG search functionality
- `POST /api/rag/upload` - Document upload and processing

### RAG Features
- Web search integration
- Document similarity search
- Content summarization
- Entity extraction
- Keyword extraction
- Multi-format file support

## Development Notes

### GPU Acceleration
The system automatically detects and uses MPS (Metal Performance Shaders) on Apple Silicon Macs for faster model inference:
```python
device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
```

### Dependencies Management
Key dependencies include:
- `fastapi`, `uvicorn` for API framework
- `chromadb[httpx]` for vector database
- `langchain-chroma`, `langchain-huggingface` for RAG
- `sentence-transformers` for embeddings
- `torch` with MPS support
- `transformers` for AI models

### Docker Services
- **PostgreSQL**: Database with custom initialization
- **Redis**: Caching and session management
- **ChromaDB**: Vector storage with REST API
- **Frontend**: Next.js application
- **Backend**: FastAPI application (run locally for development)

## Troubleshooting

### Common Issues
1. **Port Conflicts**: ChromaDB uses port 8000, run backend on 8001
2. **ChromaDB Connection**: Ensure Docker container is healthy
3. **GPU Memory**: MPS acceleration requires sufficient GPU memory
4. **Dependencies**: Use `pip install "chromadb[httpx]"` with quotes

### Health Checks
Check service status:
```bash
docker ps  # View running containers
docker logs owlynn-chroma  # Check ChromaDB logs
docker logs owlynn-postgres  # Check database logs
```

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test thoroughly
5. Submit a pull request

## License

[Add your license information here]

## Project Structure

```
Owlynn/
â”œâ”€â”€ backend/                      # Backend application
â”‚   â”œâ”€â”€ api/                     # API endpoints and routes
â”‚   â”‚   â”œâ”€â”€ rag.py              # RAG-specific endpoints
â”‚   â”‚   â”œâ”€â”€ conversation.py     # Conversation endpoints
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ core/                    # Core application components
â”‚   â”‚   â”œâ”€â”€ models/             # Data models
â”‚   â”‚   â”œâ”€â”€ schemas/            # Pydantic schemas
â”‚   â”‚   â””â”€â”€ config/             # Configuration
â”‚   â”œâ”€â”€ services/               # Business logic services
â”‚   â”‚   â”œâ”€â”€ rag/               # RAG-related services
â”‚   â”‚   â”‚   â”œâ”€â”€ search.py      # RAG search service
â”‚   â”‚   â”‚   â”œâ”€â”€ processor.py   # Content processor
â”‚   â”‚   â”‚   â””â”€â”€ cache.py       # Cache service
â”‚   â”‚   â”œâ”€â”€ chat/              # Chat-related services
â”‚   â”‚   â”‚   â”œâ”€â”€ conversation.py # Conversation service
â”‚   â”‚   â”‚   â”œâ”€â”€ memory.py      # Memory service
â”‚   â”‚   â”‚   â””â”€â”€ enhancer.py    # Chat enhancer
â”‚   â”‚   â”œâ”€â”€ processing/        # Content processing services
â”‚   â”‚   â”‚   â”œâ”€â”€ file.py        # File processor
â”‚   â”‚   â”‚   â”œâ”€â”€ image.py       # Image processor
â”‚   â”‚   â”‚   â””â”€â”€ code.py        # Code processor
â”‚   â”‚   â””â”€â”€ search/            # Search-related services
â”‚   â”‚       â”œâ”€â”€ web.py         # Web search service
â”‚   â”‚       â””â”€â”€ history.py     # Search history service
â”‚   â”œâ”€â”€ db/                     # Database related code
â”‚   â”‚   â”œâ”€â”€ migrations/        # Database migrations
â”‚   â”‚   â””â”€â”€ models/            # Database models
â”‚   â”œâ”€â”€ vectorstore/           # Vector store implementation
â”‚   â”œâ”€â”€ utils/                 # Utility functions
â”‚   â””â”€â”€ tests/                 # Backend tests
â”œâ”€â”€ frontend/                   # Frontend application
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/        # React components
â”‚   â”‚   â”œâ”€â”€ pages/            # Page components
â”‚   â”‚   â”œâ”€â”€ services/         # Frontend services
â”‚   â”‚   â””â”€â”€ utils/            # Frontend utilities
â”‚   â””â”€â”€ public/               # Static assets
â”œâ”€â”€ docs/                      # Documentation
â”‚   â”œâ”€â”€ api/                  # API documentation
â”‚   â”œâ”€â”€ setup/               # Setup guides
â”‚   â””â”€â”€ architecture/        # Architecture docs
â”œâ”€â”€ data/                     # Data storage
â”‚   â”œâ”€â”€ uploads/             # Uploaded files
â”‚   â””â”€â”€ chroma/              # ChromaDB data
â”œâ”€â”€ scripts/                  # Utility scripts
â”œâ”€â”€ docker/                   # Docker configuration
â”‚   â”œâ”€â”€ backend/             # Backend Dockerfile
â”‚   â””â”€â”€ frontend/            # Frontend Dockerfile
â”œâ”€â”€ docker-compose.yml        # Docker compose configuration
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ README.md                # Project documentation
```

# Project Name

## Overview
This project is a FastAPI application that integrates with ChromaDB for data storage and retrieval. It also includes a frontend for user interaction.

## Setup
1. **Clone the Repository**: Clone the repository to your local machine.
2. **Install Dependencies**: Navigate to the project directory and install the required dependencies using `pip install -r requirements.txt`.
3. **Run the Application**: Use the command `uvicorn backend.main:app --reload --host 0.0.0.0 --port 8001` to start the FastAPI application.

## Dependencies
- FastAPI
- Uvicorn
- ChromaDB
- Newspaper3k
- Other dependencies listed in `requirements.txt`

## Running the Application
- **Backend**: Start the FastAPI application using the command provided in the setup section.
- **Frontend**: Ensure the frontend is configured to communicate with the backend API.

## Testing
- Use the Swagger UI at `http://localhost:8001/docs` to test the API endpoints.
